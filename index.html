<!DOCTYPE html>
<html>

<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
	<meta name="theme-color" content="#33474d">
	<title>Hexo</title>
	<link rel="stylesheet" href="/css/style.css" />
	
      <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
    
<meta name="generator" content="Hexo 6.2.0"></head>

<body>

	<header class="header">
		<nav class="header__nav">
			
				<a href="/archives" class="header__link">Archive</a>
			
				<a href="/tags" class="header__link">Tags</a>
			
				<a href="/atom.xml" class="header__link">RSS</a>
			
		</nav>
		<h1 class="header__title"><a href="/">Hexo</a></h1>
		<h2 class="header__subtitle"></h2>
	</header>

	<main>
		



	<article>
	
		<h1><a href="/2022/06/17/Kafka命令行操作/">《Kafka环境配置》</a></h1>
	
	<div class="article__infos">
		<span class="article__date">2022-06-17</span><br />
		
		
			<span class="article__tags">
			  	<a class="article__tag-none-link" href="/tags/kafka/" rel="tag">kafka</a>
			</span>
		
	</div>

	

	
		<p>Welcome to you</p>
<h2 id="Kafka命令行操作"><a href="#Kafka命令行操作" class="headerlink" title="Kafka命令行操作"></a>Kafka命令行操作</h2><h3 id="四部分"><a href="#四部分" class="headerlink" title="四部分"></a>四部分</h3><h4 id="第一步：Kafka命令行工具的基本概念"><a href="#第一步：Kafka命令行工具的基本概念" class="headerlink" title="第一步：Kafka命令行工具的基本概念"></a>第一步：Kafka命令行工具的基本概念</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">kafka-topics.sh 查看帮助</span><br><span class="line">kafka-topics.sh --list --zookeeper node1:2181,node2:2181,node3:2181（啥都没有） </span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="第二步：Kafka命令行topic操作"><a href="#第二步：Kafka命令行topic操作" class="headerlink" title="第二步：Kafka命令行topic操作"></a>第二步：Kafka命令行topic操作</h4><h5 id="（1）创建-topic"><a href="#（1）创建-topic" class="headerlink" title="（1）创建 topic"></a>（1）创建 topic</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">基本方式</span><br><span class="line">./kafka-topics.sh --zookeeper node1:2181,node2:2181,node3:2181 --create --replication-factor 3 --partitions 3 --topic test</span><br><span class="line"></span><br><span class="line">--replication-factor 副本数量</span><br><span class="line">--partitions 分区数量</span><br><span class="line">--topic topic 名称</span><br><span class="line">手动指定副本的存储位置</span><br><span class="line">bin/kafka-topics.sh --create --topic tpc_1 --zookeeper node1:2181 --replica-assignment 0:1,1:2</span><br><span class="line">该方式下,命令会自动判断所要创建的 topic 的分区数及副本数</span><br><span class="line">--replica-assignment 不能同时使用--partitions --replication-factor参数。指定partition的AR列表，未指定AR列表则会根据负载均衡算法将partition的replica均衡的分布在Kafka集群中。</span><br><span class="line">--replica-assignment 1:3,2:1,3:2，逗号区分不同的partition，冒号区别相同partition中的replica，partition-0的AR=[1,3]，partition-1的AR=[2,1]，partition-2的AR=[3,2]。</span><br><span class="line">Eg：testMcdull222AR列表计算出来时--replica-assignment 2:3,1:3,1:2 。</span><br><span class="line">--replica-assignment 参数一般不由用户指定，由Kafka默认分配算法保证，有两个原则：</span><br><span class="line">（1）使Topic的所有Partition Replica能够均匀地分配至各个Kafka Broker（负载均衡）；</span><br><span class="line">（2）Partition 内的replica能够均匀地分配在不同Kafka Broker。如果Partition的第一个Replica分配至某一个Kafka Broker，那么这个Partition的其它Replica则需要分配至其它的Kafka Brokers，即Partition Replica分配至不同的Broker；</span><br><span class="line">1、从Broker随机位置开始按照轮询方式选择每个Partition的第一个replica</span><br><span class="line">2、不同Partition剩余replica按照一定的偏移量紧跟着各自的第一个replica</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h5 id="（2）删除-topic"><a href="#（2）删除-topic" class="headerlink" title="（2）删除 topic"></a>（2）删除 topic</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">bin/kafka-topics.sh  --delete --topic tpc_1 --zookeeper node1：2181</span><br><span class="line"></span><br><span class="line">（异步线程去删除）删除 topic,需要一个参数处于启用状态: delete.topic.enable = true,否则删不掉</span><br><span class="line"></span><br><span class="line">使用 kafka-topics .sh 脚本删除主题的行为本质上只是在 ZooKeeper 中的 /admin/delete_topics 路径下 建一个与待删除主题同名的节点,以标记该主题为待删除的状态。与创建主题相同的是,真正删除主题的动作也是由 Kafka 的控制器负责完成的。</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h5 id="（3）查看-topic"><a href="#（3）查看-topic" class="headerlink" title="（3）查看 topic"></a>（3）查看 topic</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">(1)列出当前系统中的所有 topic </span><br><span class="line">bin/kafka-topics.sh --zookeeper node1:2181,node2:2181,node3:2181 –list</span><br><span class="line">(2)查看 topic 详细信息</span><br><span class="line">bin/kafka-topics.sh --create --topic tpc_1   --zookeeper node1:2181 --replica-assignment 0:1,1:2</span><br><span class="line">bin/kafka-topics.sh --describe --topic tpc_1 --zookeper node1:2181 </span><br><span class="line">	 Topic: tpc_1 PartitionCount:2 ReplicationFactor:2 Configs: </span><br><span class="line">	 Topic: tpc_1 Partition: 0 Leader: 0 Replicas: 0,1 Isr: 0,1</span><br><span class="line">	 Topic: tpc_1 Partition: 1 Leader: 1 Replicas: 1,2 Isr: 1,2</span><br><span class="line"></span><br><span class="line">从上面的结果中, 可以看出, topic 的分区数量, 以及每个分区的副本数量, 以及每个副本所在的 broker 节点,以及每个分区的 leader 副本所在 broker 节点,以及每个分区的 ISR 副本列表; </span><br><span class="line"></span><br><span class="line">ISR: in sync replicas 同步副本(当然也包含 leader 自身) </span><br><span class="line">OSR:out of sync replicas 失去同步的副本(数据与 leader 之间的差距超过配置的阈值)</span><br><span class="line">ISR:In Sync Replica</span><br><span class="line">kafka不是完全同步，也不是完全异步</span><br><span class="line">1.leader会维持一个与其保持同步的replica集合，该集合就是ISR，每一个partition都有一个ISR，它时有leader动态维护。</span><br><span class="line"></span><br><span class="line">2.我们要保证kafka不丢失message，就要保证ISR这组集合存活（至少有一个存活），并且消息commit成功。</span><br><span class="line"></span><br><span class="line">AR：  Assigned Replicas的缩写，是每个partition下所有副本（replicas）的统称；</span><br><span class="line">ISR： In-Sync Replicas的缩写，是指副本同步队列，ISR是AR中的一个子集；</span><br><span class="line"></span><br><span class="line">LEO：LogEndOffset的缩写，表示每个partition的log最后一条Message的位置。</span><br><span class="line">HW： HighWatermark的缩写，是指consumer能够看到的此partition的位置。 取一个partition对应的ISR中最小的LEO作为HW，consumer最多只能消费到HW所在的位置。</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h5 id="（4）增加分区数"><a href="#（4）增加分区数" class="headerlink" title="（4）增加分区数"></a>（4）增加分区数</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --alter --topic tpc_1 --partitions 3 --zookeeper node1:2181</span><br><span class="line"></span><br><span class="line">Kafka 只支持增加分区,不支持减少分区</span><br><span class="line">原因是:减少分区,代价太大(数据的转移,日志段拼接合并) </span><br><span class="line"></span><br><span class="line">如果真的需要实现此功能,则完全可以重新创建一个分区数较小的主题,然后将现有主题中的消息按照既定的逻辑复制过去;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h5 id="（5）动态配置-topic-参数"><a href="#（5）动态配置-topic-参数" class="headerlink" title="（5）动态配置 topic 参数"></a>（5）动态配置 topic 参数</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">通过管理命令,可以为已创建的 topic 增加、修改、删除 topic level 参数</span><br><span class="line"></span><br><span class="line">添加、修改配置参数（开启压缩发送传输种提高kafka消息吞吐量的有效办法(‘gzip’, ‘snappy’, ‘lz4’, ‘zstd’)）</span><br><span class="line">bin/kafka-configs.sh --zookeeper node1:2181 --entity-type topics --entity-name tpc_1 --alter --add-config compression.type=gzip </span><br><span class="line"></span><br><span class="line">删除配置参数</span><br><span class="line">bin/kafka-configs.sh --zookeeper node1:2181 --entity-type topics --entity-name tpc_1 --alter --delete-config compression.type</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="第三步：Kafka命令行生产者与消费者操作"><a href="#第三步：Kafka命令行生产者与消费者操作" class="headerlink" title="第三步：Kafka命令行生产者与消费者操作"></a>第三步：Kafka命令行生产者与消费者操作</h4><h5 id="（1）生产者-kafka-console-producer"><a href="#（1）生产者-kafka-console-producer" class="headerlink" title="（1）生产者:kafka-console-producer"></a>（1）生产者:kafka-console-producer</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">bin/kafka-console-producer.sh --broker-list node1:9092, node2:9092, node3:9092 --topic tpc_1</span><br><span class="line"></span><br><span class="line">&gt;hello word </span><br><span class="line">&gt;kafka </span><br><span class="line">&gt;nihao</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h5 id="（2）消费者-kafka-console-consumer"><a href="#（2）消费者-kafka-console-consumer" class="headerlink" title="（2）消费者:kafka-console-consumer"></a>（2）消费者:kafka-console-consumer</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">(1)消费消息</span><br><span class="line">bin/kafka-console-consumer.sh --bootstrap-server node1:9092, node2:9092, node1:9092 --topic tpc_1 --from-beginning(从头开始)</span><br><span class="line"></span><br><span class="line">(2)#指定要消费的分区,和要消费的起始 offset </span><br><span class="line">bin/kafka-console-consumer.sh --bootstrap-server node1:9092,node2:9092,node3:9092 --topic tcp_1 --offset 2 --partition 0</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="第四步：Kafka命令行配置管理"><a href="#第四步：Kafka命令行配置管理" class="headerlink" title="第四步：Kafka命令行配置管理"></a>第四步：Kafka命令行配置管理</h4><h5 id="配置管理-kafka-configs"><a href="#配置管理-kafka-configs" class="headerlink" title="配置管理 kafka-configs"></a>配置管理 kafka-configs</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">kafka-configs.sh 脚本是专门用来对配置进行操作的, 这里的操作是运行状态修改原有的配置, 如此可以达到动态变更的目; </span><br><span class="line"></span><br><span class="line">kafka-configs.sh 脚本包含:变更 alter、查看 describe 这两种指令类型。同使用 kafka-topics.sh 脚本变更配置一样,增、删、改的行为都可以看做变更操作,不过 kafka-configs. sh 脚本不仅可支持操作主题相关的配置,还支持操 broker 、用户和客户端这 3 个类型的配置。</span><br><span class="line"></span><br><span class="line">kafka-configs.sh 脚本使用 entity-type 参数来指定操作配置的类型, 并且使 entity-name 参数来指定操作配置的名称。</span><br><span class="line"></span><br><span class="line">比如查看 topic 的配置可以按如下方式执行:</span><br><span class="line"> bin/kafka-configs.sh zookeeper node1: 2181 --describe --entity-type topics --entity-name tpc_2 </span><br><span class="line"></span><br><span class="line">比如查看 broker 的动态配置可以按如下方式执行:</span><br><span class="line"> bin/kafka-configs.sh zookeeper node1: 2181 --describe --entity-type brokers --entity-name 0 --zookeeper node1:2181</span><br><span class="line"></span><br><span class="line">示例:添加 topic 级别参数</span><br><span class="line">bin/kafka-configs.sh --zookeeper localhost:2181 --alter --entity-type topics --entity-name tpc_2 --add-config cleanup.policy=compact , max.message.bytes=10000</span><br><span class="line">使用 kafka-configs.sh 脚本来变更( alter )配置时,会在 ZooKeeper 中创建一个命名形式为: /config/&lt;entity-type&gt;/&lt;ent ity name &gt;的节点,并将变更的配置写入这个节点</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<h4 id="作者"><a href="#作者" class="headerlink" title="作者"></a>作者</h4><p>Author: lixiaolong<br>Email: <a href="mailto:&#x32;&#x39;&#x37;&#49;&#x37;&#x32;&#x36;&#x34;&#56;&#x33;&#x40;&#x71;&#x71;&#x2e;&#x63;&#x6f;&#x6d;">&#x32;&#x39;&#x37;&#49;&#x37;&#x32;&#x36;&#x34;&#56;&#x33;&#x40;&#x71;&#x71;&#x2e;&#x63;&#x6f;&#x6d;</a><br>&#96;&#96;&#96;</p>

	

	

</article>




	<article>
	
		<h1><a href="/2022/06/17/Kafka环境配置/">《Kafka环境配置》</a></h1>
	
	<div class="article__infos">
		<span class="article__date">2022-06-17</span><br />
		
		
			<span class="article__tags">
			  	<a class="article__tag-none-link" href="/tags/kafka/" rel="tag">kafka</a>
			</span>
		
	</div>

	

	
		<p>Welcome to you</p>
<h2 id="Kafka环境配置"><a href="#Kafka环境配置" class="headerlink" title="Kafka环境配置"></a>Kafka环境配置</h2><h3 id="zookeeper环境配置"><a href="#zookeeper环境配置" class="headerlink" title="zookeeper环境配置"></a>zookeeper环境配置</h3><h4 id="第一步：下载zookeeeper的压缩包，下载网址如下"><a href="#第一步：下载zookeeeper的压缩包，下载网址如下" class="headerlink" title="第一步：下载zookeeeper的压缩包，下载网址如下"></a>第一步：下载zookeeeper的压缩包，下载网址如下</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">http://archive.apache.org/dist/zookeeper/</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="第二步：解压"><a href="#第二步：解压" class="headerlink" title="第二步：解压"></a>第二步：解压</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd /export/server</span><br><span class="line"># 在node1主机上，解压zookeeper的压缩包到/export/server路径下去，然后准备进行安装</span><br><span class="line">tar -zxvf zookeeper-3.4.6.tar.gz </span><br><span class="line">ln -s zookeeper-3.4.6/ zookeeper</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="第三步：修改配置文件"><a href="#第三步：修改配置文件" class="headerlink" title="第三步：修改配置文件"></a>第三步：修改配置文件</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">在node1主机上，修改配置文件</span><br><span class="line"></span><br><span class="line">cd /export/server/zookeeper/conf/</span><br><span class="line">cp zoo_sample.cfg zoo.cfg</span><br><span class="line">mkdir -p /export/server/zookeeper/zkdatas/</span><br><span class="line">vim  zoo.cfg</span><br><span class="line">将zoo.cfg修改为以下内容</span><br><span class="line">#Zookeeper的数据存放目录</span><br><span class="line">dataDir=/export/server/zookeeper/zkdatas</span><br><span class="line"></span><br><span class="line"># 保留多少个快照</span><br><span class="line">autopurge.snapRetainCount=3</span><br><span class="line"></span><br><span class="line"># 日志多少小时清理一次</span><br><span class="line">autopurge.purgeInterval=1</span><br><span class="line"></span><br><span class="line"># 集群中服务器地址</span><br><span class="line">server.1=node1:2888:3888</span><br><span class="line">server.2=node2:2888:3888</span><br><span class="line">server.3=node3:2888:3888</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="第四步：添加myid配置"><a href="#第四步：添加myid配置" class="headerlink" title="第四步：添加myid配置"></a>第四步：添加myid配置</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">在node1主机的/export/server/zookeeper/zkdatas/这个路径下创建一个文件，文件名为myid ,文件内容为1</span><br><span class="line">echo 1 &gt; /export/server/zookeeper/zkdatas/myid</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="第五步：安装包分发并修改myid的值"><a href="#第五步：安装包分发并修改myid的值" class="headerlink" title="第五步：安装包分发并修改myid的值"></a>第五步：安装包分发并修改myid的值</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">在node1主机上，将安装包分发到其他机器第一台机器上面执行以下两个命令</span><br><span class="line">cd /export/server/</span><br><span class="line">scp -r /export/server/zookeeper-3.4.6/ node2:$PWD </span><br><span class="line">scp -r /export/server/zookeeper-3.4.6/ node3:$PWD</span><br><span class="line"></span><br><span class="line">第二台机器上建立软连接, 并修改myid的值为2</span><br><span class="line">cd /export/server/</span><br><span class="line">ln -s zookeeper-3.4.6/ zookeeper </span><br><span class="line">echo 2 &gt; /export/server/zookeeper/zkdatas/myid</span><br><span class="line"></span><br><span class="line">第三台机器上建立软连接, 并修改myid的值为3</span><br><span class="line">cd /export/server/</span><br><span class="line">ln -s zookeeper-3.4.6/ zookeeper </span><br><span class="line">echo 3 &gt; /export/server/zookeeper/zkdatas/myid</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="第六步：三台机器启动zookeeper服务"><a href="#第六步：三台机器启动zookeeper服务" class="headerlink" title="第六步：三台机器启动zookeeper服务"></a>第六步：三台机器启动zookeeper服务</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">三台机器分别启动zookeeper服务、这个命令三台机器都要执行</span><br><span class="line">/export/server/zookeeper/bin/zkServer.sh start</span><br><span class="line">/export/server/zookeeper/bin/zkServer.sh stop</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="配置环境变量"><a href="#配置环境变量" class="headerlink" title="配置环境变量"></a>配置环境变量</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">vim /etc/profile </span><br><span class="line">export ZOOKEEPER_HOME=/export/server/zookeeper</span><br><span class="line">export PATH=$PATH:$ZOOKEEPER_HOME/bin</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="zookeeper环境配置-1"><a href="#zookeeper环境配置-1" class="headerlink" title="zookeeper环境配置"></a>zookeeper环境配置</h3><h4 id="第一步：上传安装包"><a href="#第一步：上传安装包" class="headerlink" title="第一步：上传安装包"></a>第一步：上传安装包</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">移动到指定文件夹</span><br><span class="line">mv kafka_2.11-2.0.0.tgz /export/server</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="第二步：解压将文件上传到-x2F-export-x2F-server-里面-，解压"><a href="#第二步：解压将文件上传到-x2F-export-x2F-server-里面-，解压" class="headerlink" title="第二步：解压将文件上传到 &#x2F;export&#x2F;server 里面 ，解压"></a>第二步：解压将文件上传到 &#x2F;export&#x2F;server 里面 ，解压</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf kafka_2.11-2.0.0.tgz</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="第二步：修改配置文件"><a href="#第二步：修改配置文件" class="headerlink" title="第二步：修改配置文件"></a>第二步：修改配置文件</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(1)进入配置文件目录cd /export/server/kafka_2.11-2.0.0/config</span><br><span class="line">(2)编辑配置文件</span><br></pre></td></tr></table></figure>

<h4 id="第三步：添加环境变量"><a href="#第三步：添加环境变量" class="headerlink" title="第三步：添加环境变量"></a>第三步：添加环境变量</h4><p>vi server.properties </p>
<p>#为依次增长的:0、1、2、3、4,集群中唯一 id<br>broker.id&#x3D;0 –》从0开始，每台不能重复</p>
<p>—-socket server setting——-</p>
<p>Listeners &#x3D; plaintext:&#x2F;&#x2F;:9092通信用的是明文传递（加密ssl 改成：Listeners &#x3D; plaintext:&#x2F;&#x2F;node1:9092 —》拓展netstat -ntlp  8020&#x2F;9020 namenode的rpc连接端口</p>
<p>Advertised.listeners–》云服务的配置（目前不用管）</p>
<p>Socket.request.max.bytes –》一个请求最多接收多少数据，不能超过这么大，会被拒绝</p>
<p>—-Logbasic——<br>#数据存储的目录<br>log.dirs&#x3D;&#x2F;export&#x2F;server&#x2F;data&#x2F;kafka-logs </p>
<p>#默认分区数<br>Num.partitions &#x3D; 1</p>
<p>—-Log flush policy—-<br>数据安全性（持久化之前先放到缓存上，从缓存刷到磁盘上）interval.messages     interval.ms</p>
<p>—-Log retention policy—-<br>数据保留策略 168&#x2F;24&#x3D;7，1073741824&#x2F;1024&#x3D;1GB，300000ms &#x3D; 300s &#x3D; 5min超过了删掉<br>（最后修改时间还是创建时间-》日志段中最晚的一条消息，维护这个最大的时间戳–》用户无法干预）<br>—zookeeper—-<br>#指定 zk 集群地址<br>zookeeper.connect&#x3D;node1:2181,node2:2181,node3:2181<br>—-group coordinator setting 组协调设置—–<br>Reblance–》3s，3000ms 重新负载的等待延迟–》太小了会一直在负载（等3s再负载，网络抖动等）</p>
<h4 id="第四步：分发kafka"><a href="#第四步：分发kafka" class="headerlink" title="第四步：分发kafka"></a>第四步：分发kafka</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /export/server/</span><br><span class="line">scp -r /export/server/kafka_2.11-2.0.0/ node2:$PWD</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="第五步：配置环境变量"><a href="#第五步：配置环境变量" class="headerlink" title="第五步：配置环境变量"></a>第五步：配置环境变量</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/profile </span><br><span class="line">export KAFKA_HOME=/export/server/kafka </span><br><span class="line">export PATH=$PATH:$KAFKA_HOME/bin </span><br><span class="line">source /etc/profile </span><br><span class="line">注意:还需要分发环境变量</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="第六步：分别在-node2-和-node3-上修改配置文件"><a href="#第六步：分别在-node2-和-node3-上修改配置文件" class="headerlink" title="第六步：分别在 node2 和 node3 上修改配置文件"></a>第六步：分别在 node2 和 node3 上修改配置文件</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">broker.id=1 broker.id=2(broker.id 不能重复)</span><br><span class="line">Listeners = plaintext://node2:9092 /Listeners = plaintext://node3:9092 </span><br><span class="line">kafka-server-start.sh -daemon /export/server/kafka/config/server.properties </span><br><span class="line">停止集群</span><br><span class="line">kafka-server-stop.sh stop</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h4 id="作者"><a href="#作者" class="headerlink" title="作者"></a>作者</h4><p>Author: lixiaolong<br>Email: <a href="mailto:&#x32;&#x39;&#x37;&#49;&#x37;&#x32;&#x36;&#x34;&#56;&#x33;&#64;&#x71;&#x71;&#x2e;&#x63;&#111;&#x6d;">&#x32;&#x39;&#x37;&#49;&#x37;&#x32;&#x36;&#x34;&#56;&#x33;&#64;&#x71;&#x71;&#x2e;&#x63;&#111;&#x6d;</a><br>&#96;&#96;&#96;</p>

	

	

</article>




	<article>
	
		<h1><a href="/2022/06/17/kafka API使用方法/">《Kafka环境配置》</a></h1>
	
	<div class="article__infos">
		<span class="article__date">2022-06-17</span><br />
		
		
			<span class="article__tags">
			  	<a class="article__tag-none-link" href="/tags/kafka/" rel="tag">kafka</a>
			</span>
		
	</div>

	

	
		<p>Welcome to you</p>
<h2 id="kafka-API使用方法"><a href="#kafka-API使用方法" class="headerlink" title="kafka API使用方法"></a>kafka API使用方法</h2><h3 id="三部分"><a href="#三部分" class="headerlink" title="三部分"></a>三部分</h3><h4 id="第一步：生产者api参数发送方式（发后即忘）"><a href="#第一步：生产者api参数发送方式（发后即忘）" class="headerlink" title="第一步：生产者api参数发送方式（发后即忘）"></a>第一步：生产者api参数发送方式（发后即忘）</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">创建生产者实例和构建消息之后 就可以开始发送消息了。发送消息主要有 3 种模式:</span><br><span class="line"></span><br><span class="line">发后即忘( fire-and-forget) --&gt;天文领域</span><br><span class="line">发后即忘,它只管往 Kafka 发送,并不关心消息是否正确到达。</span><br><span class="line">在大多数情况下,这种发送方式没有问题; </span><br><span class="line">不过在某些时候(比如发生不可重试异常时)会造成消息的丢失。</span><br><span class="line">这种发送方式的性能最高,可靠性最差。</span><br><span class="line">Future&lt;RecordMetadata&gt; send = producer.send(rcd);--》也是异步</span><br><span class="line">没成功的话，producer也不管了</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="第二步：生产者api参数发送方式（同步发送）"><a href="#第二步：生产者api参数发送方式（同步发送）" class="headerlink" title="第二步：生产者api参数发送方式（同步发送）"></a>第二步：生产者api参数发送方式（同步发送）</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">同步发送(sync ) </span><br><span class="line">try &#123;</span><br><span class="line">	producer.send(rcd).get( ); --》一旦调用get方法，就会阻塞</span><br><span class="line">&#125; catch (Exception e) &#123; </span><br><span class="line">	e.printStackTrace( );</span><br><span class="line"> &#125;</span><br><span class="line">0.8.x 前,有一个参数 producer.type=sycn|asycn 来决定生产者的发送模式;</span><br><span class="line">现已失效(其实，新版中,producer 在底层只有异步方式，若想同步，发送一次，get一次就可实现)</span><br><span class="line"></span><br><span class="line">Future  future = Callable.run( )  有返回值，future.get（）</span><br><span class="line">runnable.run（）无返回值</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="第三步：生产者api参数发送方式（异步发送）"><a href="#第三步：生产者api参数发送方式（异步发送）" class="headerlink" title="第三步：生产者api参数发送方式（异步发送）"></a>第三步：生产者api参数发送方式（异步发送）</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">异步发送(async ) </span><br><span class="line">回调函数会在 producer 收到 ack 时调用,为异步调用,该方法有两个参数,分别是 RecordMetadata 和Exception,如果 Exception 为 null,说明消息发送成功,如果 Exception 不为 null,说明消息发送失败。</span><br><span class="line"></span><br><span class="line">注意:消息发送失败会自动重试,不需要我们在回调函数中手动重试。</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h4><h5 id="1-Kafka生产者api示例"><a href="#1-Kafka生产者api示例" class="headerlink" title="1.Kafka生产者api示例"></a>1.Kafka生产者api示例</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">一个正常的生产逻辑需要具备以下几个步骤</span><br><span class="line">(1)配置生产者客户端参数及创建相应的生产者实例</span><br><span class="line">(2)构建待发送的消息</span><br><span class="line">(3)发送消息</span><br><span class="line">(4)关闭生产者实例</span><br><span class="line">Producer java api</span><br><span class="line">首先,引入 maven 依赖</span><br><span class="line">&lt;dependency&gt; </span><br><span class="line">	&lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; </span><br><span class="line">	&lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; </span><br><span class="line">	&lt;version&gt;2.0.0&lt;/version&gt; </span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">采用默认分区方式将消息散列的发送到各个分区当中</span><br><span class="line">import org.apache.kafka.clients.producer.KafkaProducer; </span><br><span class="line">import org.apache.kafka.clients.producer.Producer; </span><br><span class="line">import org.apache.kafka.clients.producer.ProducerRecord; </span><br><span class="line">import java.util.Properties; </span><br><span class="line">public class MyProducer &#123; </span><br><span class="line">public static void main(String[ ] args) throws InterruptedException &#123; </span><br><span class="line">Properties props = new Properties(); </span><br><span class="line">//设置 kafka 集群的地址\</span><br><span class="line">props.put(&quot;bootstrap.servers&quot;, &quot;node1:9092,node2:9092,node3:9092&quot;);</span><br><span class="line">//ack 模式,取值有 0,1,-1(all) , all 是最慢但最安全的，</span><br><span class="line">0不等响应就继续发（可靠性低），1leader会写到本地日志后，然后给响应，producer拿到响应才继续发（follwer还没同步）</span><br><span class="line">props.put(“acks”, “all”); --》很重要</span><br><span class="line">props.put(“retries”, 3); //失败重试次数-&gt;失败会自动重试（可恢复/不可恢复）--&gt;(有可能会造成数据的乱序)</span><br><span class="line">props.put(“batch.size”, 10); //数据发送的批次大小提高效率/吞吐量太大会数据延迟</span><br><span class="line">props.put(&quot;linger.ms&quot;, 10000); //消息在缓冲区保留的时间,超过设置的值就会被提交到服务端</span><br><span class="line">props.put(&quot;max.request.size&quot;,10); //数据发送请求的最大缓存数</span><br><span class="line">props.put(“buffer.memory”, 10240); //整个 Producer 用到总内存的大小,如果缓冲区满了会提交数据到服务端</span><br><span class="line">//buffer.memory 要大于 batch.size,否则会报申请内存不足的错误降低阻塞的可能性</span><br><span class="line">props.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;); //key-value序列化器</span><br><span class="line">props.put(“value.serializer”, “org.apache.kafka.common.serialization.StringSerializer”);字符串最好</span><br><span class="line">Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props); </span><br><span class="line">for (int i = 0; i &lt; 100; i++) </span><br><span class="line">producer.send(new ProducerRecord&lt;String, String&gt;(&quot;test&quot;, Integer.toString(i), &quot;dd:&quot;+i)); //Thread.sleep(1000000); producer.close(); &#125; &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h5 id="2-必要参数配置"><a href="#2-必要参数配置" class="headerlink" title="2.必要参数配置"></a>2.必要参数配置</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">消息对象 ProducerRecord,它并不是单纯意义上的消息,它包含了多个属性,原本需要发送的与业务关的消息体只是其中的一个 value 属性 ,比“ Hello, rgzn!&quot;只是 ProducerRecord 对象的一个属性。 ProducerRecord 类的定义如下:</span><br><span class="line">public class ProducerRecord&lt;K, V&gt; &#123; </span><br><span class="line">	private final String topic; </span><br><span class="line">	private final Integer partition;</span><br><span class="line">	private final Headers headers; </span><br><span class="line">	private final K key; </span><br><span class="line">	private final V value; </span><br><span class="line">	private final Long timestamp;</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<h4 id="作者"><a href="#作者" class="headerlink" title="作者"></a>作者</h4><p>Author: lixiaolong<br>Email: <a href="mailto:&#x32;&#57;&#x37;&#49;&#55;&#50;&#54;&#52;&#x38;&#51;&#64;&#x71;&#113;&#x2e;&#99;&#111;&#109;">&#x32;&#57;&#x37;&#49;&#55;&#50;&#54;&#52;&#x38;&#51;&#64;&#x71;&#113;&#x2e;&#99;&#111;&#109;</a><br>&#96;&#96;&#96;</p>

	

	

</article>




	<article>
	
		<h1><a href="/2022/05/28/Spark/">Spark基础环境配置</a></h1>
	
	<div class="article__infos">
		<span class="article__date">2022-05-28</span><br />
		
		
			<span class="article__tags">
			  	<a class="article__tag-none-link" href="/tags/Spark/" rel="tag">Spark</a>
			</span>
		
	</div>

	

	
		<p>Welcome to you </p>
<h2 id="基础配置"><a href="#基础配置" class="headerlink" title="基础配置"></a>基础配置</h2><h3 id="完成对虚拟的主机名、网卡的设置"><a href="#完成对虚拟的主机名、网卡的设置" class="headerlink" title="完成对虚拟的主机名、网卡的设置"></a>完成对虚拟的主机名、网卡的设置</h3><table>
<thead>
<tr>
<th>主机名</th>
<th>node1.itcast.cn</th>
<th>node2.itcast.cn</th>
<th>node3.itcast.cn</th>
</tr>
</thead>
<tbody><tr>
<td>IP</td>
<td>192.168.88.151</td>
<td>192.168.88.152</td>
<td>192.168.88.153</td>
</tr>
<tr>
<td>用户名、密码</td>
<td>root  123456</td>
<td>root  123456</td>
<td>root  123456</td>
</tr>
</tbody></table>
<h3 id="相关操作"><a href="#相关操作" class="headerlink" title="相关操作"></a>相关操作</h3><h4 id="查看主机名"><a href="#查看主机名" class="headerlink" title="查看主机名"></a>查看主机名</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /etc/hostname</span><br></pre></td></tr></table></figure>

<h4 id="查看hosts映射"><a href="#查看hosts映射" class="headerlink" title="查看hosts映射"></a>查看hosts映射</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /etc/hosts</span><br></pre></td></tr></table></figure>

<h4 id="查看防火墙状态"><a href="#查看防火墙状态" class="headerlink" title="查看防火墙状态"></a>查看防火墙状态</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl status firewalld.service</span><br></pre></td></tr></table></figure>

<h4 id="登陆node1、node2、node3"><a href="#登陆node1、node2、node3" class="headerlink" title="登陆node1、node2、node3"></a>登陆node1、node2、node3</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ssh node1</span><br><span class="line">ssh node2</span><br><span class="line">ssh node3</span><br></pre></td></tr></table></figure>

<h4 id="同步时间"><a href="#同步时间" class="headerlink" title="同步时间"></a>同步时间</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ntpdate ntp5.aliyun.com</span><br></pre></td></tr></table></figure>

<h2 id="JDK配置"><a href="#JDK配置" class="headerlink" title="JDK配置"></a>JDK配置</h2><h3 id="编译环境软件安装目录"><a href="#编译环境软件安装目录" class="headerlink" title="编译环境软件安装目录"></a>编译环境软件安装目录</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /export/server</span><br></pre></td></tr></table></figure>

<h3 id="JDK导入-x2F-export-x2F-server，解压"><a href="#JDK导入-x2F-export-x2F-server，解压" class="headerlink" title="JDK导入&#x2F;export&#x2F;server，解压"></a>JDK导入&#x2F;export&#x2F;server，解压</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf jdk-8u241-linux-x64.tar.gz</span><br></pre></td></tr></table></figure>

<h3 id="配置环境变量"><a href="#配置环境变量" class="headerlink" title="配置环境变量"></a>配置环境变量</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br><span class="line">export JAVA_HOME=/export/server/jdk1.8.0_241</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br><span class="line">export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar</span><br></pre></td></tr></table></figure>

<h3 id="重新加载环境变量文件"><a href="#重新加载环境变量文件" class="headerlink" title="重新加载环境变量文件"></a>重新加载环境变量文件</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure>

<h3 id="查看-java-版本号"><a href="#查看-java-版本号" class="headerlink" title="查看 java 版本号"></a>查看 java 版本号</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java -version</span><br></pre></td></tr></table></figure>

<h3 id="将java拷贝并传输到所属的node2和node3"><a href="#将java拷贝并传输到所属的node2和node3" class="headerlink" title="将java拷贝并传输到所属的node2和node3"></a>将java拷贝并传输到所属的node2和node3</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp -r /export/server/jdk1.8.0_241/ root@node2:/export/server/</span><br><span class="line">scp -r /export/server/jdk1.8.0_241/ root@node3:/export/server/</span><br></pre></td></tr></table></figure>

<h3 id="配置-node2-和-node3-的-JDK-环境变量（注：和上方node1的配置方法一样）"><a href="#配置-node2-和-node3-的-JDK-环境变量（注：和上方node1的配置方法一样）" class="headerlink" title="配置 node2 和 node3 的 JDK 环境变量（注：和上方node1的配置方法一样）"></a>配置 node2 和 node3 的 JDK 环境变量（注：和上方node1的配置方法一样）</h3><h3 id="在node1-node2-和node3-创建软连接"><a href="#在node1-node2-和node3-创建软连接" class="headerlink" title="在node1 node2 和node3 创建软连接"></a>在node1 node2 和node3 创建软连接</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/server</span><br><span class="line">ln -s jdk1.8.0_241/ jdk</span><br></pre></td></tr></table></figure>

<h3 id="重新加载环境变量文件-1"><a href="#重新加载环境变量文件-1" class="headerlink" title="重新加载环境变量文件"></a>重新加载环境变量文件</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure>

<h2 id="zookeeper安装配置"><a href="#zookeeper安装配置" class="headerlink" title="zookeeper安装配置"></a>zookeeper安装配置</h2><p>注：下文中master、slave1、slave2分别为node1、node2、node3</p>
<h3 id="配置主机名和IP的映射关系，修改-x2F-etc-x2F-hosts-文件，添加-master-root-slave1-root-slave2-root"><a href="#配置主机名和IP的映射关系，修改-x2F-etc-x2F-hosts-文件，添加-master-root-slave1-root-slave2-root" class="headerlink" title="配置主机名和IP的映射关系，修改 &#x2F;etc&#x2F;hosts 文件，添加 master.root slave1.root slave2.root"></a>配置主机名和IP的映射关系，修改 &#x2F;etc&#x2F;hosts 文件，添加 master.root slave1.root slave2.root</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/hosts</span><br><span class="line">#结果显示</span><br><span class="line">127.0.0.1 localhost localhost.localdomain localhost4</span><br><span class="line">localhost4.localdomain4</span><br><span class="line">::1 localhost localhost.localdomain localhost6</span><br><span class="line">localhost6.localdomain6</span><br><span class="line">192.168.88.135 master master.root</span><br><span class="line">192.168.88.136 slave1 slave1.root</span><br><span class="line">192.168.88.137 slave2 slave2.root</span><br></pre></td></tr></table></figure>

<h3 id="zookeeper安装-上传-zookeeper-3-4-10-tar-gz到-x2F-export-x2F-server-x2F-目录下-并解压文件"><a href="#zookeeper安装-上传-zookeeper-3-4-10-tar-gz到-x2F-export-x2F-server-x2F-目录下-并解压文件" class="headerlink" title="zookeeper安装 上传 zookeeper-3.4.10.tar.gz到&#x2F;export&#x2F;server&#x2F;目录下 并解压文件"></a>zookeeper安装 上传 zookeeper-3.4.10.tar.gz到&#x2F;export&#x2F;server&#x2F;目录下 并解压文件</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/server/</span><br><span class="line">tar -zxvf zookeeper-3.4.10.tar.gz</span><br></pre></td></tr></table></figure>

<h3 id="在-x2F-export-x2F-server-目录下创建软连接"><a href="#在-x2F-export-x2F-server-目录下创建软连接" class="headerlink" title="在 &#x2F;export&#x2F;server 目录下创建软连接"></a>在 &#x2F;export&#x2F;server 目录下创建软连接</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/server</span><br><span class="line">ln -s zookeeper-3.4.10/ zookeeper</span><br></pre></td></tr></table></figure>

<h3 id="进入-x2F-export-x2F-server-x2F-zookeeper-x2F-conf-x2F-将-zoo-sample-cfg-文件复制为新文件-zoo-cfg"><a href="#进入-x2F-export-x2F-server-x2F-zookeeper-x2F-conf-x2F-将-zoo-sample-cfg-文件复制为新文件-zoo-cfg" class="headerlink" title="进入 &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;conf&#x2F; 将 zoo_sample.cfg 文件复制为新文件 zoo.cfg"></a>进入 &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;conf&#x2F; 将 zoo_sample.cfg 文件复制为新文件 zoo.cfg</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/server/zookeeper/conf/</span><br><span class="line">cp zoo_sample.cfg zoo.cfg</span><br></pre></td></tr></table></figure>

<h3 id="接上步给-zoo-cfg-添加内容"><a href="#接上步给-zoo-cfg-添加内容" class="headerlink" title="接上步给 zoo.cfg 添加内容"></a>接上步给 zoo.cfg 添加内容</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#Zookeeper的数据存放目录</span><br><span class="line">dataDir=/export/server/zookeeper/zkdatas</span><br><span class="line"># 保留多少个快照</span><br><span class="line">autopurge.snapRetainCount=3</span><br><span class="line"># 日志多少小时清理一次</span><br><span class="line">autopurge.purgeInterval=1</span><br><span class="line"># 集群中服务器地址</span><br><span class="line">server.1=master:2888:3888</span><br><span class="line">server.2=slave1:2888:3888</span><br><span class="line">server.3=slave2:2888:3888</span><br></pre></td></tr></table></figure>

<h3 id="进入-x2F-export-x2F-server-x2F-zookeeper-x2F-zkdatas-目录在此目录下创建-myid-文件，将-1-写入进去"><a href="#进入-x2F-export-x2F-server-x2F-zookeeper-x2F-zkdatas-目录在此目录下创建-myid-文件，将-1-写入进去" class="headerlink" title="进入 &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas 目录在此目录下创建 myid 文件，将 1 写入进去"></a>进入 &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas 目录在此目录下创建 myid 文件，将 1 写入进去</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /export/server/zookeeper/zkdata</span><br><span class="line">touch myid</span><br><span class="line">echo &#x27;1&#x27; &gt; myid</span><br></pre></td></tr></table></figure>

<h3 id="将-master-节点中-x2F-export-x2F-server-x2F-zookeeper-3-4-10-路径下内容推送给slave1-和-slave2"><a href="#将-master-节点中-x2F-export-x2F-server-x2F-zookeeper-3-4-10-路径下内容推送给slave1-和-slave2" class="headerlink" title="将 master 节点中&#x2F;export&#x2F;server&#x2F;zookeeper-3.4.10 路径下内容推送给slave1 和 slave2"></a>将 master 节点中&#x2F;export&#x2F;server&#x2F;zookeeper-3.4.10 路径下内容推送给slave1 和 slave2</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp -r /export/server/zookeeper-3.4.10/ slave1:$PWD</span><br><span class="line">scp -r /export/server/zookeeper-3.4.10/ slave2:$PWD</span><br></pre></td></tr></table></figure>

<h3 id="推送成功后，分别在-slave1-和-slave2-上创建软连接"><a href="#推送成功后，分别在-slave1-和-slave2-上创建软连接" class="headerlink" title="推送成功后，分别在 slave1 和 slave2 上创建软连接"></a>推送成功后，分别在 slave1 和 slave2 上创建软连接</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ln -s zookeeper-3.4.10/ zookeeper</span><br></pre></td></tr></table></figure>

<h3 id="接上步推送完成后将-slave1-和-slave2-的-x2F-export-x2F-server-x2F-zookeeper-x2F-zkdatas-x2F-文件夹下的-myid-中的内容分别改为-2-和-3"><a href="#接上步推送完成后将-slave1-和-slave2-的-x2F-export-x2F-server-x2F-zookeeper-x2F-zkdatas-x2F-文件夹下的-myid-中的内容分别改为-2-和-3" class="headerlink" title="接上步推送完成后将 slave1 和 slave2 的 &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas&#x2F; 文件夹下的 myid 中的内容分别改为 2 和 3"></a>接上步推送完成后将 slave1 和 slave2 的 &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas&#x2F; 文件夹下的 myid 中的内容分别改为 2 和 3</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cd /export/server/zookeeper/zkdatas/</span><br><span class="line">结果显示：</span><br><span class="line">[root@slave1 zkdatas]# vim myid</span><br><span class="line">[root@slave1 zkdatas]# more myid</span><br><span class="line">2</span><br><span class="line">[root@slave2 zkdatas]# vim myid</span><br><span class="line">[root@slave2 zkdatas]# more myid</span><br><span class="line">3</span><br></pre></td></tr></table></figure>

<h3 id="配置zookeeper的环境变量（注：三台主机都需要配置）"><a href="#配置zookeeper的环境变量（注：三台主机都需要配置）" class="headerlink" title="配置zookeeper的环境变量（注：三台主机都需要配置）"></a>配置zookeeper的环境变量（注：三台主机都需要配置）</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br><span class="line"># zookeeper 环境变量</span><br><span class="line">export ZOOKEEPER_HOME=/export/server/zookeeper</span><br><span class="line">export PATH=$PATH:$ZOOKEEPER_HOME/bin</span><br></pre></td></tr></table></figure>

<h3 id="重新加载环境变量文件-2"><a href="#重新加载环境变量文件-2" class="headerlink" title="重新加载环境变量文件"></a>重新加载环境变量文件</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure>

<h3 id="进入-x2F-export-x2F-server-x2F-zookeeper-3-4-10-x2F-bin-目录下启动-zkServer-sh-脚本-（注：三台都需要做）"><a href="#进入-x2F-export-x2F-server-x2F-zookeeper-3-4-10-x2F-bin-目录下启动-zkServer-sh-脚本-（注：三台都需要做）" class="headerlink" title="进入 &#x2F;export&#x2F;server&#x2F;zookeeper-3.4.10&#x2F;bin 目录下启动 zkServer.sh 脚本 （注：三台都需要做）"></a>进入 &#x2F;export&#x2F;server&#x2F;zookeeper-3.4.10&#x2F;bin 目录下启动 zkServer.sh 脚本 （注：三台都需要做）</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/server/zookeeper-3.4.10/bin</span><br><span class="line">zkServer.sh start</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">结果显示：</span><br><span class="line">[root@master bin]# ./zkServer.sh start</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /export/server/zookeeper-3.4.10/bin/../conf/zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br></pre></td></tr></table></figure>

<h3 id="查看-zookeeper-的状态"><a href="#查看-zookeeper-的状态" class="headerlink" title="查看 zookeeper 的状态"></a>查看 zookeeper 的状态</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zkServer.sh status</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">结果显示：</span><br><span class="line">[root@master server]# zkServer.sh status</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /export/server/zookeeper-3.4.10/bin/../conf/zoo.cfg</span><br><span class="line">Mode: follower</span><br><span class="line">[root@slave1 server]# zkServer.sh status</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /export/server/zookeeper-3.4.10/bin/../conf/zoo.cfg</span><br><span class="line">Mode: follower</span><br><span class="line">[root@slave2 conf]# zkServer.sh status</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /export/server/zookeeper-3.4.10/bin/../conf/zoo.cfg</span><br><span class="line">Mode: leader</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jps</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">结果显示：</span><br><span class="line">[root@master server]# jps</span><br><span class="line">125348 QuorumPeerMain</span><br><span class="line">16311 Jps</span><br><span class="line">[root@slave1 server]# jps</span><br><span class="line">126688 QuorumPeerMain</span><br><span class="line">17685 Jps</span><br><span class="line">[root@slave2 conf]# jps</span><br><span class="line">126733 QuorumPeerMain</span><br><span class="line">17727 Jps</span><br></pre></td></tr></table></figure>

<h3 id="脚本一键启动"><a href="#脚本一键启动" class="headerlink" title="脚本一键启动"></a>脚本一键启动</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">vim zkServer.sh</span><br><span class="line">#!/bin/bash</span><br><span class="line">if [ $# -eq 0 ] ;</span><br><span class="line">then</span><br><span class="line">echo &quot;please input param:start stop&quot;</span><br><span class="line">else</span><br><span class="line">if [ $1 = start ] ;then</span><br><span class="line">echo &quot;$&#123;1&#125;ing master&quot;</span><br><span class="line">ssh master &quot;source /etc/profile;/export/server/zookeeper/bin/zkServer.sh</span><br><span class="line">start&quot;</span><br><span class="line">for i in &#123;1..2&#125;</span><br><span class="line">do</span><br><span class="line">echo &quot;$&#123;1&#125;ping slave$&#123;i&#125;&quot;</span><br><span class="line">ssh slave$&#123;i&#125; &quot;source</span><br><span class="line">/etc/profile;/export/server/zookeeper/bin/zkServer.sh start&quot;</span><br><span class="line">done</span><br><span class="line">fi</span><br><span class="line">if [ $1 = stop ];then</span><br><span class="line">echo &quot;$&#123;1&#125;ping master &quot;</span><br><span class="line">ssh master &quot;source /etc/profile;/export/server/zookeeper/bin/zkServer.sh</span><br><span class="line">stop&quot;</span><br><span class="line">for i in &#123;1..2&#125;</span><br><span class="line">do</span><br><span class="line">echo &quot;$&#123;1&#125;ping slave$&#123;i&#125;&quot;</span><br><span class="line">ssh slave$&#123;i&#125; &quot;source</span><br><span class="line">/etc/profile;/export/server/zookeeper/bin/zkServer.sh stop&quot;</span><br><span class="line">done</span><br><span class="line">fi</span><br><span class="line">if [ $1 = status ];then</span><br><span class="line">echo &quot;$&#123;1&#125;ing master&quot;</span><br><span class="line">ssh master &quot;source /etc/profile;/export/server/zookeeper/bin/zkServer.sh</span><br><span class="line">status&quot;</span><br><span class="line">for i in &#123;1..2&#125;</span><br><span class="line">do</span><br><span class="line">echo &quot;$&#123;1&#125;ping slave$&#123;i&#125;&quot;</span><br><span class="line">ssh slave$&#123;i&#125; &quot;source</span><br><span class="line">/etc/profile;/export/server/zookeeper/bin/zkServer.sh status&quot;</span><br><span class="line">done</span><br><span class="line">fi</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 将文件放在 /bin 目录下</span><br><span class="line">chmod +x zkServer-all.sh &amp;&amp; zkServer-all.sh</span><br></pre></td></tr></table></figure>

<h2 id="Hadoop-安装配置"><a href="#Hadoop-安装配置" class="headerlink" title="Hadoop 安装配置"></a>Hadoop 安装配置</h2><h3 id="把-hadoop-3-3-0-Centos7-64-with-snappy-tar-gz-上传到-x2F-export-x2F-server-并解压文件"><a href="#把-hadoop-3-3-0-Centos7-64-with-snappy-tar-gz-上传到-x2F-export-x2F-server-并解压文件" class="headerlink" title="把 hadoop-3.3.0-Centos7-64-with-snappy.tar.gz 上传到 &#x2F;export&#x2F;server 并解压文件"></a>把 hadoop-3.3.0-Centos7-64-with-snappy.tar.gz 上传到 &#x2F;export&#x2F;server 并解压文件</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">把 hadoop-3.3.0-Centos7-64-with-snappy.tar.gz 上传到 /export/server 并解压文件</span><br></pre></td></tr></table></figure>

<h3 id="修改配置文件-进入路径-x2F-export-x2F-server-x2F-hadoop-3-3-0-x2F-etc-x2F-hadoop"><a href="#修改配置文件-进入路径-x2F-export-x2F-server-x2F-hadoop-3-3-0-x2F-etc-x2F-hadoop" class="headerlink" title="修改配置文件(进入路径 &#x2F;export&#x2F;server&#x2F;hadoop-3.3.0&#x2F;etc&#x2F;hadoop)"></a>修改配置文件(进入路径 &#x2F;export&#x2F;server&#x2F;hadoop-3.3.0&#x2F;etc&#x2F;hadoop)</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /export/server/hadoop-3.3.0/etc/hadoop</span><br></pre></td></tr></table></figure>

<p>hadoop-env.sh</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#文件最后添加 </span><br><span class="line">export JAVA_HOME=/export/server/jdk1.8.0_241 export HDFS_NAMENODE_USER=root export HDFS_DATANODE_USER=root export HDFS_SECONDARYNAMENODE_USER=root export YARN_RESOURCEMANAGER_USER=root export YARN_NODEMANAGER_USER=root</span><br></pre></td></tr></table></figure>

<p>core-site.xml</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- 设置默认使用的文件系统 Hadoop支持file、HDFS、GFS、ali|Amazon云等文件系统 -</span><br><span class="line">-&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">&lt;value&gt;hdfs://master:8020&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!-- 设置Hadoop本地保存数据路径 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">&lt;value&gt;/export/data/hadoop-3.3.0&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!-- 设置HDFS web UI用户身份 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt;</span><br><span class="line">&lt;value&gt;root&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!-- 整合hive 用户代理设置 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt;</span><br><span class="line">&lt;value&gt;*&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt;</span><br><span class="line">&lt;value&gt;*&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!-- 文件系统垃圾桶保存时间 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;fs.trash.interval&lt;/name&gt;</span><br><span class="line">&lt;value&gt;1440&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p>hdfs-site.xml</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- 设置SNN进程运行机器位置信息 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;</span><br><span class="line">&lt;value&gt;slave1:9868&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p>mapred-site.xml</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- 设置MR程序默认运行模式： yarn集群模式 local本地模式 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">&lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!-- MR程序历史服务地址 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;</span><br><span class="line">&lt;value&gt;master:10020&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!-- MR程序历史服务器web端地址 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;</span><br><span class="line">&lt;value&gt;master:19888&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;yarn.app.mapreduce.am.env&lt;/name&gt;</span><br><span class="line">&lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;mapreduce.map.env&lt;/name&gt;</span><br><span class="line">&lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;mapreduce.reduce.env&lt;/name&gt;</span><br><span class="line">&lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p>yarn-site.xml</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- 设置YARN集群主角色运行机器位置 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</span><br><span class="line">&lt;value&gt;master&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">&lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!-- 是否将对容器实施物理内存限制 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt;</span><br><span class="line">&lt;value&gt;false&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!-- 是否将对容器实施虚拟内存限制。 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;</span><br><span class="line">&lt;value&gt;false&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!-- 开启日志聚集 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;</span><br><span class="line">&lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!-- 设置yarn历史服务器地址 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;yarn.log.server.url&lt;/name&gt;</span><br><span class="line">&lt;value&gt;http://master:19888/jobhistory/logs&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!-- 历史日志保存的时间 7天 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt;</span><br><span class="line">&lt;value&gt;604800&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p>workers</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">master</span><br><span class="line">slave1</span><br><span class="line">slave2</span><br></pre></td></tr></table></figure>

<h3 id="分发同步hadoop安装包"><a href="#分发同步hadoop安装包" class="headerlink" title="分发同步hadoop安装包"></a>分发同步hadoop安装包</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /export/server</span><br><span class="line">scp -r hadoop-3.3.0 root@slave1:$PWD</span><br><span class="line">scp -r hadoop-3.3.0 root@slave2:$PWD</span><br></pre></td></tr></table></figure>

<h3 id="将hadoop添加到环境变量"><a href="#将hadoop添加到环境变量" class="headerlink" title="将hadoop添加到环境变量"></a>将hadoop添加到环境变量</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br><span class="line">export HADOOP_HOME=/export/server/hadoop-3.3.0</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</span><br></pre></td></tr></table></figure>

<h3 id="重新加载环境变量文件-3"><a href="#重新加载环境变量文件-3" class="headerlink" title="重新加载环境变量文件"></a>重新加载环境变量文件</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure>

<h3 id="Hadoop集群启动"><a href="#Hadoop集群启动" class="headerlink" title="Hadoop集群启动"></a>Hadoop集群启动</h3><h4 id="格式化namenode（只有首次启动需要格式化）"><a href="#格式化namenode（只有首次启动需要格式化）" class="headerlink" title="格式化namenode（只有首次启动需要格式化）"></a>格式化namenode（只有首次启动需要格式化）</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs namenode -format</span><br></pre></td></tr></table></figure>

<h4 id="脚本一键启动-1"><a href="#脚本一键启动-1" class="headerlink" title="脚本一键启动"></a>脚本一键启动</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# start-dfs.sh</span><br><span class="line">Starting namenodes on [master]</span><br><span class="line">上一次登录：五 3月 11 21:27:24 CST 2022pts/0 上</span><br><span class="line">Starting datanodes</span><br><span class="line">上一次登录：五 3月 11 21:27:32 CST 2022pts/0 上</span><br><span class="line">Starting secondary namenodes [slave1]</span><br><span class="line">上一次登录：五 3月 11 21:27:35 CST 2022pts/0 上</span><br><span class="line">[root@master ~]# start-yarn.sh</span><br><span class="line">Starting resourcemanager</span><br><span class="line">上一次登录：五 3月 11 21:27:41 CST 2022pts/0 上</span><br><span class="line">Starting nodemanagers</span><br><span class="line">上一次登录：五 3月 11 21:27:51 CST 2022pts/0 上</span><br></pre></td></tr></table></figure>

<h4 id="启动后-输入-jps-查看"><a href="#启动后-输入-jps-查看" class="headerlink" title="启动后 输入 jps 查看"></a>启动后 输入 jps 查看</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# jps</span><br><span class="line">127729 NameNode</span><br><span class="line">127937 DataNode</span><br><span class="line">14105 Jps</span><br><span class="line">128812 NodeManager</span><br><span class="line">128591 ResourceManager</span><br><span class="line">[root@slave1 hadoop]# jps</span><br><span class="line">121889 NodeManager</span><br><span class="line">121559 SecondaryNameNode</span><br><span class="line">7014 Jps</span><br><span class="line">121369 DataNode</span><br><span class="line">[root@slave2 hadoop]# jps</span><br><span class="line">6673 Jps</span><br><span class="line">121543 NodeManager</span><br><span class="line">121098 DataNode</span><br></pre></td></tr></table></figure>

<h4 id="WEB页面"><a href="#WEB页面" class="headerlink" title="WEB页面"></a>WEB页面</h4><h4 id="HDFS集群"><a href="#HDFS集群" class="headerlink" title="HDFS集群"></a>HDFS集群</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://master:9870/</span><br></pre></td></tr></table></figure>

<h4 id="YARN集群"><a href="#YARN集群" class="headerlink" title="YARN集群"></a>YARN集群</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://master:8088/</span><br></pre></td></tr></table></figure>


<h4 id="作者"><a href="#作者" class="headerlink" title="作者"></a>作者</h4><p>Author: lixiaolong<br>Email: <a href="mailto:&#x32;&#57;&#55;&#49;&#55;&#50;&#x36;&#52;&#x38;&#x33;&#64;&#113;&#x71;&#x2e;&#99;&#x6f;&#109;">&#x32;&#57;&#55;&#49;&#55;&#50;&#x36;&#52;&#x38;&#x33;&#64;&#113;&#x71;&#x2e;&#99;&#x6f;&#109;</a></p>
<pre><code>
</code></pre>

	

	

</article>




	<article>
	
		<h1><a href="/2022/05/28/Spark-HA-Yarn/">Spark HA &amp; Yarn配置</a></h1>
	
	<div class="article__infos">
		<span class="article__date">2022-05-28</span><br />
		
		
			<span class="article__tags">
			  	<a class="article__tag-none-link" href="/tags/spark/" rel="tag">spark</a>
			</span>
		
	</div>

	

	
		<p>Welcome to you</p>
<h2 id="Spark-On-YARN模式"><a href="#Spark-On-YARN模式" class="headerlink" title="Spark On YARN模式"></a>Spark On YARN模式</h2><p>在已有YARN集群的前提下在单独准备Spark StandAlone集群,对资源的利用就不高.Spark On YARN, 无 需部署Spark集群, 只要找一台服务器, 充当Spark的客户端</p>
<h3 id="保证-HADOOP-CONF-和-DIR-YARN-CONF-DIR-已经配置在spark-env-sh-和环境变量中-（注-前面配置spark-Standlone-时已经配置过此项了）"><a href="#保证-HADOOP-CONF-和-DIR-YARN-CONF-DIR-已经配置在spark-env-sh-和环境变量中-（注-前面配置spark-Standlone-时已经配置过此项了）" class="headerlink" title="保证 HADOOP_CONF_和 DIR_YARN_CONF_DIR 已经配置在spark-env.sh 和环境变量中 （注: 前面配置spark-Standlone 时已经配置过此项了）"></a>保证 HADOOP_CONF_和 DIR_YARN_CONF_DIR 已经配置在spark-env.sh 和环境变量中 （注: 前面配置spark-Standlone 时已经配置过此项了）</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">spark-env.sh 文件部分显示：</span><br><span class="line">....</span><br><span class="line">77 ## HADOOP软件配置文件目录，读取HDFS上文件和运行YARN集群</span><br><span class="line">78 HADOOP_CONF_DIR=/export/server/hadoop/etc/hadoop</span><br><span class="line">79 YARN_CONF_DIR=/export/server/hadoop/etc/hadoop</span><br><span class="line">....</span><br></pre></td></tr></table></figure>

<h3 id="链接到-YARN-中（注-交互式环境-pyspark-和-spark-shell-无法运行-cluster模式）"><a href="#链接到-YARN-中（注-交互式环境-pyspark-和-spark-shell-无法运行-cluster模式）" class="headerlink" title="链接到 YARN 中（注: 交互式环境 pyspark 和 spark-shell 无法运行 cluster模式）"></a>链接到 YARN 中（注: 交互式环境 pyspark 和 spark-shell 无法运行 cluster模式）</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bin/pyspark --master yarn --deploy-mode client|cluster</span><br><span class="line"># --deploy-mode 选项是指定部署模式, 默认是 客户端模式</span><br><span class="line"># client就是客户端模式</span><br><span class="line"># cluster就是集群模式</span><br><span class="line"># --deploy-mode 仅可以用在YARN模式下</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-shell --master yarn --deploy-mode client|cluster</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit --master yarn --deploy-mode client|cluster /xxx/xxx/xxx.py</span><br><span class="line">参数</span><br></pre></td></tr></table></figure>

<h3 id="spark-submit-和-spark-shell-和-pyspark的相关参数"><a href="#spark-submit-和-spark-shell-和-pyspark的相关参数" class="headerlink" title="spark-submit 和 spark-shell 和 pyspark的相关参数"></a>spark-submit 和 spark-shell 和 pyspark的相关参数</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">- bin/pyspark: pyspark解释器spark环境</span><br><span class="line">- bin/spark-shell: scala解释器spark环境</span><br><span class="line">- bin/spark-submit: 提交jar包或Python文件执行的工具</span><br><span class="line">- bin/spark-sql: sparksql客户端工具</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">这4个客户端工具的参数基本通用.以spark-submit 为例:</span><br><span class="line">bin/spark-submit --master spark://master:7077 xxx.py`</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line">Usage: spark-submit [options] &lt;app jar | python file | R file&gt; [app</span><br><span class="line">arguments]</span><br><span class="line">Usage: spark-submit --kill [submission ID] --master [spark://...]</span><br><span class="line">Usage: spark-submit --status [submission ID] --master [spark://...]</span><br><span class="line">Usage: spark-submit run-example [options] example-class [example args]</span><br><span class="line">Options:</span><br><span class="line">--master MASTER_URL spark://host:port, mesos://host:port, yarn,</span><br><span class="line">k8s://https://host:port, or local (Default:</span><br><span class="line">local[*]).</span><br><span class="line">--deploy-mode DEPLOY_MODE 部署模式 client 或者 cluster 默认是client</span><br><span class="line">--class CLASS_NAME 运行java或者scala class(for Java / Scala apps).</span><br><span class="line">--name NAME 程序的名字</span><br><span class="line">--jars JARS Comma-separated list of jars to include on the</span><br><span class="line">driver</span><br><span class="line">and executor classpaths.</span><br><span class="line">--packages Comma-separated list of maven coordinates of</span><br><span class="line">jars to include</span><br><span class="line">on the driver and executor classpaths. Will</span><br><span class="line">search the local</span><br><span class="line">maven repo, then maven central and any</span><br><span class="line">additional remote</span><br><span class="line">repositories given by --repositories. The</span><br><span class="line">format for the</span><br><span class="line">coordinates should be</span><br><span class="line">groupId:artifactId:version.</span><br><span class="line">--exclude-packages Comma-separated list of groupId:artifactId, to</span><br><span class="line">exclude while</span><br><span class="line">resolving the dependencies provided in --</span><br><span class="line">packages to avoid</span><br><span class="line">dependency conflicts.</span><br><span class="line">--repositories Comma-separated list of additional remote</span><br><span class="line">repositories to</span><br><span class="line">search for the maven coordinates given with --</span><br><span class="line">packages.</span><br><span class="line">--py-files PY_FILES 指定Python程序依赖的其它python文件</span><br><span class="line">--files FILES Comma-separated list of files to be placed in</span><br><span class="line">the working</span><br><span class="line">directory of each executor. File paths of</span><br><span class="line">these files</span><br><span class="line">in executors can be accessed via</span><br><span class="line">SparkFiles.get(fileName).</span><br><span class="line">--archives ARCHIVES Comma-separated list of archives to be</span><br><span class="line">extracted into the</span><br><span class="line">working directory of each executor.</span><br><span class="line">--conf, -c PROP=VALUE 手动指定配置</span><br><span class="line">--properties-file FILE Path to a file from which to load extra</span><br><span class="line">properties. If not</span><br><span class="line">specified, this will look for conf/spark defaults.conf.</span><br><span class="line">--driver-memory MEM Driver的可用内存(Default: 1024M).</span><br><span class="line">--driver-java-options Driver的一些Java选项</span><br><span class="line">--driver-library-path Extra library path entries to pass to the</span><br><span class="line">driver.</span><br><span class="line">--driver-class-path Extra class path entries to pass to the</span><br><span class="line">driver. Note that</span><br><span class="line">jars added with --jars are automatically</span><br><span class="line">included in the</span><br><span class="line">classpath.</span><br><span class="line">--executor-memory MEM Executor的内存 (Default: 1G).</span><br><span class="line">--proxy-user NAME User to impersonate when submitting the</span><br><span class="line">application.</span><br><span class="line">This argument does not work with --principal /</span><br><span class="line">--keytab.</span><br><span class="line">--help, -h 显示帮助文件</span><br><span class="line">--verbose, -v Print additional debug output.</span><br><span class="line">--version, 打印版本</span><br><span class="line">Cluster deploy mode only(集群模式专属):</span><br><span class="line">--driver-cores NUM Driver可用的的CPU核数(Default: 1).</span><br><span class="line">Spark standalone or Mesos with cluster deploy mode only:</span><br><span class="line">--supervise 如果给定, 可以尝试重启Driver</span><br><span class="line">Spark standalone, Mesos or K8s with cluster deploy mode only:</span><br><span class="line">--kill SUBMISSION_ID 指定程序ID kill</span><br><span class="line">--status SUBMISSION_ID 指定程序ID 查看运行状态</span><br><span class="line">Spark standalone, Mesos and Kubernetes only:</span><br><span class="line">--total-executor-cores NUM 整个任务可以给Executor多少个CPU核心用</span><br><span class="line">Spark standalone, YARN and Kubernetes only:</span><br><span class="line">--executor-cores NUM 单个Executor能使用多少CPU核心</span><br><span class="line">Spark on YARN and Kubernetes only(YARN模式下):</span><br><span class="line">--num-executors NUM Executor应该开启几个</span><br><span class="line">--principal PRINCIPAL Principal to be used to login to KDC.</span><br><span class="line">--keytab KEYTAB The full path to the file that contains the</span><br><span class="line">keytab for the</span><br><span class="line">principal specified above.</span><br><span class="line">Spark on YARN only:</span><br><span class="line">--queue QUEUE_NAME 指定运行的YARN队列(Default: &quot;default&quot;).</span><br></pre></td></tr></table></figure>

<h3 id="启动-YARN-的历史服务器"><a href="#启动-YARN-的历史服务器" class="headerlink" title="启动 YARN 的历史服务器"></a>启动 YARN 的历史服务器</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/server/hadoop-3.3.0/sbin</span><br><span class="line">./mr-jobhistory-daemon.sh start historyserver</span><br></pre></td></tr></table></figure>

<h3 id="访问WebUI界面"><a href="#访问WebUI界面" class="headerlink" title="访问WebUI界面"></a>访问WebUI界面</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://master:19888/</span><br></pre></td></tr></table></figure>

<h3 id="client-模式测试"><a href="#client-模式测试" class="headerlink" title="client 模式测试"></a>client 模式测试</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">SPARK_HOME=/export/server/spark</span><br><span class="line">$&#123;SPARK_HOME&#125;/bin/spark-submit --master yarn --deploy-mode client --</span><br><span class="line">driver-memory 512m --executor-memory 512m --num-executors 1 --total executor-cores 2 $&#123;SPARK_HOME&#125;/examples/src/main/python/pi.py 3</span><br></pre></td></tr></table></figure>

<h3 id="cluster-模式测试"><a href="#cluster-模式测试" class="headerlink" title="cluster 模式测试"></a>cluster 模式测试</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">SPARK_HOME=/export/server/spark</span><br><span class="line">$&#123;SPARK_HOME&#125;/bin/spark-submit --master yarn --deploy-mode cluster --driver-memory 512m --executor-memory 512m --num-executors 1 --total-executor-cores</span><br><span class="line">2 --conf &quot;spark.pyspark.driver.python=/root/anaconda3/bin/python3&quot; --conf</span><br><span class="line">&quot;spark.pyspark.python=/root/anaconda3/bin/python3&quot;</span><br><span class="line">$&#123;SPARK_HOME&#125;/examples/src/main/python/pi.py 3</span><br></pre></td></tr></table></figure>


<h4 id="作者"><a href="#作者" class="headerlink" title="作者"></a>作者</h4><p>Author: lixiaolong<br>Email: <a href="mailto:&#50;&#57;&#x37;&#x31;&#55;&#50;&#54;&#x34;&#x38;&#51;&#64;&#113;&#x71;&#x2e;&#x63;&#x6f;&#x6d;">&#50;&#57;&#x37;&#x31;&#55;&#50;&#54;&#x34;&#x38;&#51;&#64;&#113;&#x71;&#x2e;&#x63;&#x6f;&#x6d;</a></p>
<pre><code>
</code></pre>

	

	

</article>




	<article>
	
		<h1><a href="/2022/05/28/Spark-local-stand-alone/">Spark local&amp; stand-alone配置</a></h1>
	
	<div class="article__infos">
		<span class="article__date">2022-05-28</span><br />
		
		
			<span class="article__tags">
			  	<a class="article__tag-none-link" href="/tags/spark/" rel="tag">spark</a>
			</span>
		
	</div>

	

	
		<p>Welcome to you</p>
<h2 id="Spark安装配置"><a href="#Spark安装配置" class="headerlink" title="Spark安装配置"></a>Spark安装配置</h2><h3 id="Spark-local模式"><a href="#Spark-local模式" class="headerlink" title="Spark-local模式"></a>Spark-local模式</h3><h4 id="Anaconda-On-Linux-安装-单台服务器脚本安装"><a href="#Anaconda-On-Linux-安装-单台服务器脚本安装" class="headerlink" title="Anaconda On Linux 安装 (单台服务器脚本安装)"></a>Anaconda On Linux 安装 (单台服务器脚本安装)</h4><h4 id="安装上传安装包-资料中提供的Anaconda3-2021-05-Linux-x86-64-sh文件到Linux服务器上安装-位置在-x2F-export-x2F-server"><a href="#安装上传安装包-资料中提供的Anaconda3-2021-05-Linux-x86-64-sh文件到Linux服务器上安装-位置在-x2F-export-x2F-server" class="headerlink" title="安装上传安装包: 资料中提供的Anaconda3-2021.05-Linux-x86_64.sh文件到Linux服务器上安装 位置在 &#x2F;export&#x2F;server"></a>安装上传安装包: 资料中提供的Anaconda3-2021.05-Linux-x86_64.sh文件到Linux服务器上安装 位置在 &#x2F;export&#x2F;server</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /export/server</span><br><span class="line"># 运行文件</span><br><span class="line">sh Anaconda3-2021.05-Linux-x86_64.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">过程显示：</span><br><span class="line">...</span><br><span class="line"># 出现内容选 yes</span><br><span class="line">Please answer &#x27;yes&#x27; or &#x27;no&#x27;:&#x27;</span><br><span class="line">&gt;&gt;&gt; yes</span><br><span class="line">...</span><br><span class="line"># 出现添加路径：/export/server/anaconda3</span><br><span class="line">...</span><br><span class="line">[/root/anaconda3] &gt;&gt;&gt; /export/server/anaconda3</span><br><span class="line">PREFIX=/export/server/anaconda3</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<h4 id="安装完成后-退出终端，-重新进来"><a href="#安装完成后-退出终端，-重新进来" class="headerlink" title="安装完成后, 退出终端， 重新进来"></a>安装完成后, 退出终端， 重新进来</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">exit</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">结果显示：</span><br><span class="line"># 看到这个Base开头表明安装好了.base是默认的虚拟环境.</span><br><span class="line">Last login: Tue Mar 15 15:28:59 2022 from 192.168.88.1</span><br><span class="line">(base) [root@master ~]#</span><br></pre></td></tr></table></figure>

<h4 id="创建虚拟环境-pyspark-基于-python3-8"><a href="#创建虚拟环境-pyspark-基于-python3-8" class="headerlink" title="创建虚拟环境 pyspark 基于 python3.8"></a>创建虚拟环境 pyspark 基于 python3.8</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create -n pyspark python=3.8</span><br></pre></td></tr></table></figure>

<h4 id="切换到虚拟环境内"><a href="#切换到虚拟环境内" class="headerlink" title="切换到虚拟环境内"></a>切换到虚拟环境内</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda activate pyspark</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">结果显示：</span><br><span class="line">(base) [root@master ~]# conda activate pyspark</span><br><span class="line">(pyspark) [root@master ~]#</span><br></pre></td></tr></table></figure>

<h4 id="在虚拟环境内安装包-（有WARNING不用管）"><a href="#在虚拟环境内安装包-（有WARNING不用管）" class="headerlink" title="在虚拟环境内安装包 （有WARNING不用管）"></a>在虚拟环境内安装包 （有WARNING不用管）</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install pyhive pyspark jieba -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br></pre></td></tr></table></figure>

<h4 id="spark-安装"><a href="#spark-安装" class="headerlink" title="spark 安装"></a>spark 安装</h4><h4 id="将文件上传到-x2F-export-x2F-server-里面-，解压"><a href="#将文件上传到-x2F-export-x2F-server-里面-，解压" class="headerlink" title="将文件上传到 &#x2F;export&#x2F;server 里面 ，解压"></a>将文件上传到 &#x2F;export&#x2F;server 里面 ，解压</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /export/server</span><br><span class="line"># 解压</span><br><span class="line">tar -zxvf spark-3.2.0-bin-hadoop3.2.tgz -C /export/server/</span><br></pre></td></tr></table></figure>

<h4 id="建立软连接"><a href="#建立软连接" class="headerlink" title="建立软连接"></a>建立软连接</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ln -s /export/server/spark-3.2.0-bin-hadoop3.2 /export/server/spark</span><br></pre></td></tr></table></figure>

<h4 id="添加环境变量"><a href="#添加环境变量" class="headerlink" title="添加环境变量"></a>添加环境变量</h4><p>SPARK_HOME: 表示Spark安装路径在哪里 </p>
<p>PYSPARK_PYTHON: 表示Spark想运行Python程序, 那么去哪里找python执行器 </p>
<p>JAVA_HOME: 告知Spark Java在哪里 </p>
<p>HADOOP_CONF_DIR: 告知Spark Hadoop的配置文件在哪里 </p>
<p>HADOOP_HOME: 告知Spark Hadoop安装在哪里</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br><span class="line">内容：</span><br><span class="line">.....</span><br><span class="line">注：此部分之前配置过，此部分不需要在配置</span><br><span class="line">#JAVA_HOME</span><br><span class="line">export JAVA_HOME=/export/server/jdk1.8.0_241</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br><span class="line">export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar</span><br><span class="line">#HADOOP_HOME</span><br><span class="line">export HADOOP_HOME=/export/server/hadoop-3.3.0</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</span><br><span class="line">#ZOOKEEPER_HOME</span><br><span class="line">export ZOOKEEPER_HOME=/export/server/zookeeper</span><br><span class="line">export PATH=$PATH:$ZOOKEEPER_HOME/bin</span><br><span class="line">.....</span><br><span class="line"># 将以下部分添加进去</span><br><span class="line">#SPARK_HOME</span><br><span class="line">export SPARK_HOME=/export/server/spark</span><br><span class="line">#HADOOP_CONF_DIR</span><br><span class="line">export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop</span><br><span class="line">#PYSPARK_PYTHON</span><br><span class="line">export PYSPARK_PYTHON=/export/server/anaconda3/envs/pyspark/bin/python</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">vim .bashrc</span><br><span class="line">内容添加进去：</span><br><span class="line">#JAVA_HOME</span><br><span class="line">export JAVA_HOME=/export/server/jdk1.8.0_241</span><br><span class="line">#PYSPARK_PYTHON</span><br><span class="line">export PYSPARK_PYTHON=/export/server/anaconda3/envs/pyspark/bin/python</span><br></pre></td></tr></table></figure>

<h4 id="重新加载环境变量文件"><a href="#重新加载环境变量文件" class="headerlink" title="重新加载环境变量文件"></a>重新加载环境变量文件</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br><span class="line">source ~/.bashrc</span><br></pre></td></tr></table></figure>

<h4 id="进入-x2F-export-x2F-server-x2F-anaconda3-x2F-envs-x2F-pyspark-x2F-bin-x2F-文件夹"><a href="#进入-x2F-export-x2F-server-x2F-anaconda3-x2F-envs-x2F-pyspark-x2F-bin-x2F-文件夹" class="headerlink" title="进入 &#x2F;export&#x2F;server&#x2F;anaconda3&#x2F;envs&#x2F;pyspark&#x2F;bin&#x2F; 文件夹"></a>进入 &#x2F;export&#x2F;server&#x2F;anaconda3&#x2F;envs&#x2F;pyspark&#x2F;bin&#x2F; 文件夹</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /export/server/anaconda3/envs/pyspark/bin/</span><br></pre></td></tr></table></figure>

<h4 id="开启"><a href="#开启" class="headerlink" title="开启"></a>开启</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./pyspark</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">结果显示：</span><br><span class="line">(base) [root@master bin]# ./pyspark</span><br><span class="line">Python 3.8.12 (default, Oct 12 2021, 13:49:34)</span><br><span class="line">[GCC 7.5.0] :: Anaconda, Inc. on linux</span><br><span class="line">Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.</span><br><span class="line">Setting default log level to &quot;WARN&quot;.</span><br><span class="line">To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use</span><br><span class="line">setLogLevel(newLevel).</span><br><span class="line">2022-03-15 20:37:04,612 WARN util.NativeCodeLoader: Unable to load nativehadoop library for your platform... using builtin-java classes where</span><br><span class="line">applicable</span><br><span class="line">Welcome to</span><br><span class="line">____ __</span><br><span class="line">/ __/__ ___ _____/ /__</span><br><span class="line">_\ \/ _ \/ _ `/ __/ &#x27;_/</span><br><span class="line">/__ / .__/\_,_/_/ /_/\_\ version 3.2.0</span><br><span class="line">/_/</span><br><span class="line">Using Python version 3.8.12 (default, Oct 12 2021 13:49:34)</span><br><span class="line">Spark context Web UI available at http://master:4040</span><br><span class="line">Spark context available as &#x27;sc&#x27; (master = local[*], app id = local1647347826262).</span><br><span class="line">SparkSession available as &#x27;spark&#x27;.</span><br><span class="line">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure>

<h4 id="查看WebUI界面"><a href="#查看WebUI界面" class="headerlink" title="查看WebUI界面"></a>查看WebUI界面</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">浏览器访问：</span><br><span class="line">http://master:4040/</span><br></pre></td></tr></table></figure>

<h4 id="退出"><a href="#退出" class="headerlink" title="退出"></a>退出</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda deactivate</span><br></pre></td></tr></table></figure>

<h3 id="Spark-Standalone模式"><a href="#Spark-Standalone模式" class="headerlink" title="Spark-Standalone模式"></a>Spark-Standalone模式</h3><p>Standalone模式(集群) Spark中的各个角色以独立进程的形式存在,并组成Spark集群环境</p>
<h4 id="Anaconda-On-Linux-安装-单台服务器脚本安装-注：在-slave1-和-slave2-上部署"><a href="#Anaconda-On-Linux-安装-单台服务器脚本安装-注：在-slave1-和-slave2-上部署" class="headerlink" title="Anaconda On Linux 安装 (单台服务器脚本安装 注：在 slave1 和 slave2 上部署)"></a>Anaconda On Linux 安装 (单台服务器脚本安装 注：在 slave1 和 slave2 上部署)</h4><h4 id="安装上传安装包-资料中提供的Anaconda3-2021-05-Linux-x86-64-sh文件到Linux服务器上安装-位置在-x2F-export-x2F-server-1"><a href="#安装上传安装包-资料中提供的Anaconda3-2021-05-Linux-x86-64-sh文件到Linux服务器上安装-位置在-x2F-export-x2F-server-1" class="headerlink" title="安装上传安装包: 资料中提供的Anaconda3-2021.05-Linux-x86_64.sh文件到Linux服务器上安装 位置在 &#x2F;export&#x2F;server"></a>安装上传安装包: 资料中提供的Anaconda3-2021.05-Linux-x86_64.sh文件到Linux服务器上安装 位置在 &#x2F;export&#x2F;server</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /export/server</span><br><span class="line"># 运行文件</span><br><span class="line">sh Anaconda3-2021.05-Linux-x86_64.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">过程显示：</span><br><span class="line">...</span><br><span class="line"># 出现内容选 yes</span><br><span class="line">Please answer &#x27;yes&#x27; or &#x27;no&#x27;:&#x27;</span><br><span class="line">&gt;&gt;&gt; yes</span><br><span class="line">...</span><br><span class="line"># 出现添加路径：/export/server/anaconda3</span><br><span class="line">...</span><br><span class="line">[/root/anaconda3] &gt;&gt;&gt; /export/server/anaconda3</span><br><span class="line">PREFIX=/export/server/anaconda3</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<h4 id="安装完成后-退出终端，-重新进来-1"><a href="#安装完成后-退出终端，-重新进来-1" class="headerlink" title="安装完成后, 退出终端， 重新进来"></a>安装完成后, 退出终端， 重新进来</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">exit</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">结果显示：</span><br><span class="line"># 看到这个Base开头表明安装好了.base是默认的虚拟环境.</span><br><span class="line">Last login: Tue Mar 15 15:28:59 2022 from 192.168.88.1</span><br><span class="line">(base) [root@master ~]#</span><br></pre></td></tr></table></figure>

<h4 id="在-master-节点上把-x2F-bashrc-和-profile-分发给-slave1-和-slave2"><a href="#在-master-节点上把-x2F-bashrc-和-profile-分发给-slave1-和-slave2" class="headerlink" title="在 master 节点上把 .&#x2F;bashrc 和 profile 分发给 slave1 和 slave2"></a>在 master 节点上把 .&#x2F;bashrc 和 profile 分发给 slave1 和 slave2</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#分发 .bashrc :</span><br><span class="line">scp ~/.bashrc root@slave1:~/</span><br><span class="line">scp ~/.bashrc root@slave2:~/</span><br><span class="line">#分发 profile :</span><br><span class="line">scp /etc/profile/ root@slave1:/etc/</span><br><span class="line">scp /etc/profile/ root@slave2:/etc/</span><br></pre></td></tr></table></figure>

<h4 id="创建虚拟环境-pyspark-基于-python3-8-1"><a href="#创建虚拟环境-pyspark-基于-python3-8-1" class="headerlink" title="创建虚拟环境 pyspark 基于 python3.8"></a>创建虚拟环境 pyspark 基于 python3.8</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create -n pyspark python=3.8</span><br></pre></td></tr></table></figure>

<h4 id="切换到虚拟环境内-1"><a href="#切换到虚拟环境内-1" class="headerlink" title="切换到虚拟环境内"></a>切换到虚拟环境内</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda activate pyspark</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">结果显示：</span><br><span class="line">(base) [root@master ~]# conda activate pyspark</span><br><span class="line">(pyspark) [root@master ~]#</span><br></pre></td></tr></table></figure>

<h4 id="在虚拟环境内安装包-（有WARNING不用管）-1"><a href="#在虚拟环境内安装包-（有WARNING不用管）-1" class="headerlink" title="在虚拟环境内安装包 （有WARNING不用管）"></a>在虚拟环境内安装包 （有WARNING不用管）</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install pyhive pyspark jieba -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br></pre></td></tr></table></figure>

<h4 id="master-节点节点进入-x2F-export-x2F-server-x2F-spark-x2F-conf-修改以下配置文件"><a href="#master-节点节点进入-x2F-export-x2F-server-x2F-spark-x2F-conf-修改以下配置文件" class="headerlink" title="master 节点节点进入 &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf 修改以下配置文件"></a>master 节点节点进入 &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf 修改以下配置文件</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /export/server/spark/conf</span><br></pre></td></tr></table></figure>

<h4 id="将文件-workers-template-改名为-workers，并配置文件内容"><a href="#将文件-workers-template-改名为-workers，并配置文件内容" class="headerlink" title="将文件 workers.template 改名为 workers，并配置文件内容"></a>将文件 workers.template 改名为 workers，并配置文件内容</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mv workers.template workers</span><br><span class="line">vim workers</span><br><span class="line"># localhost删除，内容追加文末：</span><br><span class="line">master</span><br><span class="line">slave1</span><br><span class="line">slave2</span><br><span class="line"># 功能: 这个文件就是指示了 当前SparkStandAlone环境下, 有哪些worker</span><br></pre></td></tr></table></figure>

<h4 id="将文件-spark-env-sh-template-改名为-spark-env-sh，并配置相关内容"><a href="#将文件-spark-env-sh-template-改名为-spark-env-sh，并配置相关内容" class="headerlink" title="将文件 spark-env.sh.template 改名为 spark-env.sh，并配置相关内容"></a>将文件 spark-env.sh.template 改名为 spark-env.sh，并配置相关内容</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mv spark-env.sh.template spark-env.sh</span><br><span class="line">vim spark-env.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">文末追加内容：</span><br><span class="line">## 设置JAVA安装目录</span><br><span class="line">JAVA_HOME=/export/server/jdk</span><br><span class="line">## HADOOP软件配置文件目录，读取HDFS上文件和运行YARN集群</span><br><span class="line">HADOOP_CONF_DIR=/export/server/hadoop/etc/hadoop</span><br><span class="line">YARN_CONF_DIR=/export/server/hadoop/etc/hadoop</span><br><span class="line">## 指定spark老大Master的IP和提交任务的通信端口</span><br><span class="line"># 告知Spark的master运行在哪个机器上</span><br><span class="line">export SPARK_MASTER_HOST=master</span><br><span class="line"># 告知sparkmaster的通讯端口</span><br><span class="line">export SPARK_MASTER_PORT=7077</span><br><span class="line"># 告知spark master的 webui端口</span><br><span class="line">SPARK_MASTER_WEBUI_PORT=8080</span><br><span class="line"># worker cpu可用核数</span><br><span class="line">SPARK_WORKER_CORES=1</span><br><span class="line"># worker可用内存</span><br><span class="line">SPARK_WORKER_MEMORY=1g</span><br><span class="line"># worker的工作通讯地址</span><br><span class="line">SPARK_WORKER_PORT=7078</span><br><span class="line"># worker的 webui地址</span><br><span class="line">SPARK_WORKER_WEBUI_PORT=8081</span><br><span class="line">## 设置历史服务器</span><br><span class="line"># 配置的意思是 将spark程序运行的历史日志 存到hdfs的/sparklog文件夹中</span><br><span class="line">SPARK_HISTORY_OPTS=&quot;-</span><br><span class="line">Dspark.history.fs.logDirectory=hdfs://master:8020/sparklog/ -</span><br><span class="line">Dspark.history.fs.cleaner.enabled=true&quot;</span><br></pre></td></tr></table></figure>

<h4 id="开启-hadoop-的-hdfs-和-yarn-集群"><a href="#开启-hadoop-的-hdfs-和-yarn-集群" class="headerlink" title="开启 hadoop 的 hdfs 和 yarn 集群"></a>开启 hadoop 的 hdfs 和 yarn 集群</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">start-dfs.sh</span><br><span class="line">start-yarn.sh</span><br></pre></td></tr></table></figure>

<h4 id="在HDFS上创建程序运行历史记录存放的文件夹，同样-conf-文件目录下"><a href="#在HDFS上创建程序运行历史记录存放的文件夹，同样-conf-文件目录下" class="headerlink" title="在HDFS上创建程序运行历史记录存放的文件夹，同样 conf 文件目录下"></a>在HDFS上创建程序运行历史记录存放的文件夹，同样 conf 文件目录下</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -mkdir /sparklog</span><br><span class="line">hadoop fs -chmod 777 /sparklog</span><br></pre></td></tr></table></figure>

<h4 id="将-spark-defaults-conf-template-改为-spark-defaults-conf-并做相关配置"><a href="#将-spark-defaults-conf-template-改为-spark-defaults-conf-并做相关配置" class="headerlink" title="将 spark-defaults.conf.template 改为 spark-defaults.conf 并做相关配置"></a>将 spark-defaults.conf.template 改为 spark-defaults.conf 并做相关配置</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mv spark-defaults.conf.template spark-defaults.conf</span><br><span class="line">vim spark-defaults.conf</span><br><span class="line">文末追加内容为：</span><br><span class="line"># 开启spark的日期记录功能</span><br><span class="line">spark.eventLog.enabled true</span><br><span class="line"># 设置spark日志记录的路径</span><br><span class="line">spark.eventLog.dir hdfs://master:8020/sparklog/</span><br><span class="line"># 设置spark日志是否启动压缩</span><br><span class="line">spark.eventLog.compress true</span><br></pre></td></tr></table></figure>

<h4 id="配置-log4j-properties-文件-将文件第-19-行的-log4j-rootCategory-x3D-INFO-console-改为-log4j-rootCategory-x3D-WARN-console-（即将INFO-改为-WARN-目的：输出日志-设置级别为-WARN-只输出警告和错误日志，INFO-则为输出所有信息，多数为无用信息）"><a href="#配置-log4j-properties-文件-将文件第-19-行的-log4j-rootCategory-x3D-INFO-console-改为-log4j-rootCategory-x3D-WARN-console-（即将INFO-改为-WARN-目的：输出日志-设置级别为-WARN-只输出警告和错误日志，INFO-则为输出所有信息，多数为无用信息）" class="headerlink" title="配置 log4j.properties 文件 将文件第 19 行的 log4j.rootCategory&#x3D;INFO, console 改为 log4j.rootCategory&#x3D;WARN, console （即将INFO 改为 WARN 目的：输出日志, 设置级别为 WARN 只输出警告和错误日志，INFO 则为输出所有信息，多数为无用信息）"></a>配置 log4j.properties 文件 将文件第 19 行的 log4j.rootCategory&#x3D;INFO, console 改为 log4j.rootCategory&#x3D;WARN, console （即将INFO 改为 WARN 目的：输出日志, 设置级别为 WARN 只输出警告和错误日志，INFO 则为输出所有信息，多数为无用信息）</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mv log4j.properties.template log4j.properties</span><br><span class="line">vim log4j.properties</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">结果显示：</span><br><span class="line">...</span><br><span class="line">18 # Set everything to be logged to the console</span><br><span class="line">19 log4j.rootCategory=WARN, console</span><br><span class="line">....</span><br></pre></td></tr></table></figure>

<h4 id="master-节点分发-spark-安装文件夹-到-slave1-和-slave2-上"><a href="#master-节点分发-spark-安装文件夹-到-slave1-和-slave2-上" class="headerlink" title="master 节点分发 spark 安装文件夹 到 slave1 和 slave2 上"></a>master 节点分发 spark 安装文件夹 到 slave1 和 slave2 上</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /export/server/</span><br><span class="line">scp -r /export/server/spark-3.2.0-bin-hadoop3.2/ slave1:$PWD</span><br><span class="line">scp -r /export/server/spark-3.2.0-bin-hadoop3.2/ slave2:$PWD</span><br></pre></td></tr></table></figure>

<h4 id="在slave1-和-slave2-上做软连接"><a href="#在slave1-和-slave2-上做软连接" class="headerlink" title="在slave1 和 slave2 上做软连接"></a>在slave1 和 slave2 上做软连接</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ln -s /export/server/spark-3.2.0-bin-hadoop3.2 /export/server/spark</span><br></pre></td></tr></table></figure>

<h4 id="重新加载环境变量"><a href="#重新加载环境变量" class="headerlink" title="重新加载环境变量"></a>重新加载环境变量</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure>

<h4 id="进入-x2F-export-x2F-server-x2F-spark-x2F-sbin-文件目录下-启动-start-history-server-sh"><a href="#进入-x2F-export-x2F-server-x2F-spark-x2F-sbin-文件目录下-启动-start-history-server-sh" class="headerlink" title="进入 &#x2F;export&#x2F;server&#x2F;spark&#x2F;sbin 文件目录下 启动 start-history-server.sh"></a>进入 &#x2F;export&#x2F;server&#x2F;spark&#x2F;sbin 文件目录下 启动 start-history-server.sh</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/server/spark/sbin</span><br><span class="line">./start-history-server.sh</span><br></pre></td></tr></table></figure>

<h4 id="访问-WebUI-界面"><a href="#访问-WebUI-界面" class="headerlink" title="访问 WebUI 界面"></a>访问 WebUI 界面</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">浏览器访问：</span><br><span class="line">http://master:18080/</span><br></pre></td></tr></table></figure>

<h4 id="启动Spark的Master和Worker进程"><a href="#启动Spark的Master和Worker进程" class="headerlink" title="启动Spark的Master和Worker进程"></a>启动Spark的Master和Worker进程</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># 启动全部master和worker</span><br><span class="line">sbin/start-all.sh</span><br><span class="line"># 或者可以一个个启动:</span><br><span class="line"># 启动当前机器的master</span><br><span class="line">sbin/start-master.sh</span><br><span class="line"># 启动当前机器的worker</span><br><span class="line">sbin/start-worker.sh</span><br><span class="line"># 停止全部</span><br><span class="line">sbin/stop-all.sh</span><br><span class="line"># 停止当前机器的master</span><br><span class="line">sbin/stop-master.sh</span><br><span class="line"># 停止当前机器的worker</span><br><span class="line">sbin/stop-worker.sh</span><br></pre></td></tr></table></figure>

<h4 id="访问-WebUI界面"><a href="#访问-WebUI界面" class="headerlink" title="访问 WebUI界面"></a>访问 WebUI界面</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">浏览器访问：</span><br><span class="line">http://master:8080/</span><br></pre></td></tr></table></figure>

<h4 id="作者"><a href="#作者" class="headerlink" title="作者"></a>作者</h4><p>Author: lixiaolong<br>Email: <a href="mailto:&#x32;&#57;&#55;&#x31;&#55;&#x32;&#54;&#x34;&#56;&#x33;&#x40;&#x71;&#113;&#x2e;&#99;&#111;&#x6d;">&#x32;&#57;&#55;&#x31;&#55;&#x32;&#54;&#x34;&#56;&#x33;&#x40;&#x71;&#113;&#x2e;&#99;&#111;&#x6d;</a><br>&#96;&#96;&#96;</p>

	

	

</article>






	<span class="different-posts">📕 end of posts 📕</span>


	</main>

	<footer class="footer">
	<div class="footer-content">
		
	      <div class="footer__element">
	<p>Hi there, <br />welcome to my Blog glad you found it. Have a look around, will you?</p>
</div>

	    
	      <div class="footer__element">
	<h5>Check out</h5>
	<ul class="footer-links">
		<li class="footer-links__link"><a href="/archives">Archive</a></li>
		
		  <li class="footer-links__link"><a href="/atom.xml">RSS</a></li>
	    
		<li class="footer-links__link"><a href="/about">about page</a></li>
		<li class="footer-links__link"><a href="/tags">Tags</a></li>
		<li class="footer-links__link"><a href="/categories">Categories</a></li>
	</ul>
</div>

	    

		<div class="footer-credit">
			<span>© 2022 John Doe | Powered by <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a> | Theme <a target="_blank" rel="noopener" href="https://github.com/HoverBaum/meilidu-hexo">MeiliDu</a></span>
		</div>

	</div>


</footer>



</body>

</html>
